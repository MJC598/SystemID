{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3e8186",
   "metadata": {},
   "source": [
    "## Calculus Review\n",
    "#### Differentiation\n",
    "The key to calculus (in fact the fundamental theorem of calculus) states:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}\\int_{a}^{x} f(t)dt = f(x)\n",
    "$$\n",
    "\n",
    "or if you prefer:\n",
    "\n",
    "$$\n",
    "\\int_{a}^{b} f(x)dx = F(b) - F(a)\n",
    "$$\n",
    "\n",
    "This is important because it equates integral and differential calculus.\n",
    "\n",
    "The definition of a derivative is:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\lim_{h\\to 0}\\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "Understanding the definition of a derviative is important because computers are by nature discrete machines and thus cannot deal with continuous differentiation or integration. It instead uses approximation with h becoming incredibly small.\n",
    "\n",
    "#### Chain Rule\n",
    "Differentiation is important, but as functions get complicated, it becomes important to remember how to differentiate functions with multiple components. This is where chain rule comes into play. It is used extensively in machine learning (see backpropagation for an example). It can be defined as:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}\n",
    "$$\n",
    "\n",
    "To put this into perspective, if a mathematical model has multiple internal parameters that need to be updated (i.e. different weights) then we need to update the error calculation with respect to the individual weight. This will result in different paths and requires using the chain rule. Let's take a look at a more concrete example:\n",
    "\n",
    "#### Gradients\n",
    "Gradients are used all over in system identification. The easiest way to think about it is the partial derivative of each variable in a with a multivariable function:\n",
    "\n",
    "$$\n",
    "\\nabla f(p) = \\begin{bmatrix}\n",
    "               \\frac{\\partial f}{\\partial x_1} (p) \\\\\n",
    "               . \\\\\n",
    "               . \\\\\n",
    "               . \\\\\n",
    "               \\frac{\\partial f}{\\partial x_n} (p)\n",
    "              \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In hyperdimensional planes, this gradient is used to calculate the direction and fastest increase (or fastest decrease if the vector is negated). This is where gradient descent, hill climbing, etc. is derived.\n",
    "\n",
    "#### Optimization\n",
    "Back in Calculus I/II the term critical points was likely used a lot. These denoted maxima or minima of a function (local or global). These exist when the derivative of a function is 0. Because of the 0 slope, this means these points are either saddle points, maxima, or minima. All are important areas of study and points to watch out for in machine learning and system identification as they can create traps that our models can get stuck in.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62afe7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
